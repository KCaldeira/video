# video to midi code
"""
prompt for ChatGTP:

Here's a Python script that processes every Nth frame of a video file, calculates multiple metrics (average intensity, 
standard deviation, and entropy) for each color color_channel_name (R, G, B, and grayscale) and outputs 24 MIDI files. 
Each metric is stored in two MIDI files: one directly scaled (0-127) and another inverted (127-0).

This script will:

Extract every Nth frame from the video.
Compute various metrics for each color channel.
Map values to MIDI Control Change (CC) messages (both direct and inverted).
Save each metric in a separate MIDI file, with filenames autogenerated.

"""
import cv2
import os
import pandas as pd
import numpy as np
import re
from mido import Message, MidiFile, MidiTrack
from scipy.stats import rankdata
from collections import defaultdict

def tranpose_metric(color_channel, downscale_factor):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and restored image
    mse = np.mean((downscaled - downscaled[::-1,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse =  mse / np.var(downscaled) 
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

def reflect_metric(color_channel, downscale_factor=4):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and vertically reflected image
    mse0 = np.mean((downscaled - downscaled[::-1]) ** 2)

    # Compute mean squared error (MSE) between original and horizontally reflected image
    mse1 = np.mean((downscaled - downscaled[:,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse = (mse0 + mse1) / (2. * np.var(downscaled) )
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

def radial_symmetry_metric(color_channel, dowscale_factor):
    """
    Compute radial symmetry metric for a color channel with distance bins of width `scale_factor`.
    """
    # Step 1: Create coordinate grid
    y, x = np.indices(color_channel.shape)
    center_y = (color_channel.shape[0] / 2) - 0.5
    center_x = (color_channel.shape[1] / 2) - 0.5

    # Step 2: Compute radial distance from center for each pixel
    r = np.sqrt((x - center_x)**2 + (y - center_y)**2)

    # Step 2b: Bin distances into scale_factor-wide bins
    r_bin = (r / dowscale_factor).astype(np.int32)

    # Step 3: Compute mean value for each radial bin
    max_bin = r_bin.max()
    radial_mean = np.zeros(max_bin + 1)
    counts = np.bincount(r_bin.ravel())
    sums = np.bincount(r_bin.ravel(), weights=color_channel.ravel())

    # Avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        radial_mean[:len(sums)] = np.where(counts != 0, sums / counts, 0)

    # Optional: remove zeros or masked bins if they skew the variance
    # valid = counts > 0
    # return np.var(radial_mean[valid])

    return np.var(radial_mean)


def lines_metric(color_channel_original):
    """
    Detect straight lines in a color channel using Probabilistic Hough Transform.
    Returns metrics indicating the presence, length, and robustness of straight lines.
    
    Parameters:
    - color_channel: color channel
    - downscale_factor: Scale factor for downscaling before detection
    
    Returns:
    - line_metrics: Dictionary containing:
        - 'count': Number of lines detected (normalized 0-1)
        - 'length': Average length of detected lines (normalized 0-1)
        - 'robustness': How well the detected lines match actual edges (normalized 0-1)
    """
    # copy the color channel
    color_channel = color_channel_original.copy()
    
    # Convert to uint8 if needed and scale to 0-255 range
    if color_channel.dtype != np.uint8:
        if np.max(color_channel) > 1:
            color_channel = (color_channel / 255.0).astype(np.uint8)
        else:
            color_channel = color_channel.astype(np.uint8)

    color_channel = cv2.equalizeHist(cv2.GaussianBlur(color_channel, (21,21), 0))

    
    # Downscale the image to reduce noise and computation time
    h, w = color_channel.shape
    
    # Apply edge detection
    edges = cv2.Canny(color_channel, 50, 150, apertureSize=3)
    
    # Detect line segments using Probabilistic Hough Transform
    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=50,
                           minLineLength=100, maxLineGap=50)
    
    if lines is None:
        return 0.0 #no lines detected
    
    # Calculate metrics
    line_lengths = []
    line_robustness = []
    
    # Create a mask for edge points
    edge_points = np.where(edges > 0)
    edge_coords = np.column_stack((edge_points[1], edge_points[0]))  # (x,y) coordinates
    
    for line in lines:
        x1, y1, x2, y2 = line[0]
        
        # Calculate line length
        length = np.sqrt((x2-x1)**2 + (y2-y1)**2)
        line_lengths.append(length)
        
        # Calculate line equation: ax + by + c = 0
        a = y2 - y1
        b = x1 - x2
        c = x2*y1 - x1*y2
        
        # Calculate distance from each edge point to the line
        if len(edge_coords) > 0:
            distances = np.abs(a*edge_coords[:,0] + b*edge_coords[:,1] + c) / np.sqrt(a*a + b*b)
            
            # Count points within 2 pixels of the line
            nearby_points = np.sum(distances <= 2.0)
            
            # Normalize by line length to get points per unit length
            points_per_length = nearby_points / (length + 1e-6)  # avoid division by zero
            line_robustness.append(points_per_length)
        else:
            line_robustness.append(0.0)
    
    # Normalize metrics
    non_normalized_line_metric = np.sum(np.array(line_lengths) * np.array(line_robustness))
    
    # Return 0.0 if the metric is NaN
    if np.isnan(non_normalized_line_metric):
        return 0.0
        
    return non_normalized_line_metric

def weighted_std(values, weights):
    """
    Calculate weighted standard deviation
    
    Parameters:
    - values: array of values
    - weights: array of weights (must be same length as values)
    
    Returns:
    - weighted standard deviation
    """
    # Calculate weighted mean
    weighted_mean = np.average(values, weights=weights)
    
    # Calculate weighted variance
    weighted_variance = np.average((values - weighted_mean)**2, weights=weights)
    
    # Return standard deviation (square root of variance)
    return np.sqrt(weighted_variance)

def error_dispersion_metrics(color_channel, downscale_factor):
    """
    Detect circles in a color channel using Hough Transform.
    Returns metrics indicating the presence, size, and robustness of circles.
    
    Parameters:
    - color_channel: color channel
    - downscale_factor: Scale factor for downscaling before detection
    - downscale_factor2: Scale factor for downscaling before detection
    
    Returns:
    - non_normalized_circle_metric0: Sum of circle confidences
    - non_normalized_circle_metric2: Sum of (radius^2 * confidence)
    - non_normalized_circle_metric4: Sum of (radius^4 * confidence)
    - n_circles: Number of circles detected
    - area_circles: Total area of detected circles
    - circle_confidences: List of confidence values for each circle
    """

    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled1  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)
    restored1 = cv2.resize(downscaled1, (w, h), interpolation=cv2.INTER_LINEAR)

    xvals = np.arange(0, w)
    yvals = np.arange(0, h)

    centerx = w/2
    centery = h/2   

    X, Y = np.meshgrid(xvals, yvals)

    # what is the magnitude of the lowres error?
    # how far is the lowres error from the center of the image?
    # how far is the highres error from the center of the image?
    # how dispersed is the low res error around the center of the low res error?
    # how dispersed is the high res error around the center of the high res error?
    # _std is the standard deviation of the variability of the channel.
    # now let's see what is the standard deviation 
    info_total = np.var(color_channel)
    info_large = (restored1 - np.mean(color_channel))**2 / info_total # fraction of variance in large scale
    info_small = (color_channel - restored1)**2 / info_total # fraction of variance in small scale


    # Compute mean squared error (MSE) between original and restored image
    meanx1 = np.average( X, weights= info_large)
    meanx2 = np.average( X, weights= info_small)
 
    meany1 = np.average( Y, weights= info_large)
    meany2 = np.average( Y, weights= info_small)

    stddevx1 = weighted_std( X, info_large)
    stddevx2 = weighted_std( X, info_small)
    stddevy1 = weighted_std( Y, info_large)
    stddevy2 = weighted_std( Y, info_small)

    mnsqerror1 = np.average(info_large)
    mnsqerror2 = np.average(info_small)

    dist1 = np.sqrt((meanx1 - centerx)**2 + (meany1 - centery)**2)
    dist2 = np.sqrt((meanx2 - centerx)**2 + (meany2 - centery)**2)


    stdev1 = np.sqrt(stddevx1**2 + stddevy1**2)
    stdev2 = np.sqrt(stddevx2**2 + stddevy2**2)


    return mnsqerror1, mnsqerror2, dist1, dist2, stdev1, stdev2 


def bgr_to_hsv(b, g, r):
    """
    Convert RGB to HSV for 2D numpy arrays.
    Inputs r, g, b: 2D numpy arrays with values in [0, 255]
    Outputs h in degrees [0, 360), s and v in [0.0, 1.0]
    """
    r = r.astype(np.float32) / 255
    g = g.astype(np.float32) / 255
    b = b.astype(np.float32) / 255

    cmax = np.maximum.reduce([r, g, b])
    cmin = np.minimum.reduce([r, g, b])
    delta = cmax - cmin

    # Hue calculation
    h = np.zeros_like(cmax)

    mask = delta != 0
    r_max = (cmax == r) & mask
    g_max = (cmax == g) & mask
    b_max = (cmax == b) & mask

    h[r_max] = (60 * ((g[r_max] - b[r_max]) / delta[r_max])) % 360
    h[g_max] = (60 * ((b[g_max] - r[g_max]) / delta[g_max]) + 120)
    h[b_max] = (60 * ((r[b_max] - g[b_max]) / delta[b_max]) + 240)

    # Saturation calculation
    s = np.zeros_like(cmax)
    nonzero = cmax != 0
    s[nonzero] = delta[nonzero] / cmax[nonzero]

    # Value
    v = cmax

    return h, s, v

# Circular statistics function to compute standard deviation of angle weighted by saturation
# Hue is assumed to be 0-360 degrees, saturation is 0-1
def weighted_circular_std_deg(hue, saturation):
    """Weighted circular standard deviation in degrees"""
    angles_rad = np.deg2rad(hue)
    weights = np.array(saturation)
    z = weights * np.exp(1j * angles_rad)
    R_w = np.abs(np.sum(z) / np.sum(weights))
    return np.rad2deg(np.sqrt(-2 * np.log(R_w)))





def compute_basic_metrics(frame, downscale_factor1, downscale_factor2):
    """
    Compute different intensity-based metrics on R, G, B, and color channels.
    Returns a dictionary of results.
    """
    basic_metrics = {}
    
    b, g, r = cv2.split(frame)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    h, s, v = bgr_to_hsv(b, g, r)

    # Split into R, G, B channels
    for color_channel_name, color_channel in [("R", r), ("G", g), ("B", b),
                                ("Gray", gray),("V", v)]:
        avg_intensity = np.mean(color_channel)
        variance = np.var(color_channel)


        transpose_metric_value = tranpose_metric(color_channel, downscale_factor2) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        reflect_metric_value = reflect_metric(color_channel, downscale_factor2) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        radial_symmetry_metric_value = radial_symmetry_metric(color_channel, downscale_factor2) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        # Add line detection metrics
        line_metric_value = lines_metric(color_channel)
        # Add circle detection metrics
        mnsqerror1, mnsqerror2, dist1, dist2, stdev1, stdev2 = error_dispersion_metrics(color_channel, downscale_factor1)

        # Store values
        basic_metrics[f"{color_channel_name}_avg"] = avg_intensity
        basic_metrics[f"{color_channel_name}_var"] = variance # note that variance is total info (i.e., diff^2 relative to mean)
        basic_metrics[f"{color_channel_name}_xps"] = transpose_metric_value
        basic_metrics[f"{color_channel_name}_rfl"] = reflect_metric_value
        basic_metrics[f"{color_channel_name}_rad"] = radial_symmetry_metric_value
        basic_metrics[f"{color_channel_name}_lin"] = line_metric_value
        basic_metrics[f"{color_channel_name}_ee1"] = mnsqerror1  # mean squared error of low res
        basic_metrics[f"{color_channel_name}_ee2"] = mnsqerror2  # mean squared error of high res
        basic_metrics[f"{color_channel_name}_ed1"] = dist1  # distance of low res error from center of image
        basic_metrics[f"{color_channel_name}_ed2"] = dist2  # distance of high res error from center of image
        basic_metrics[f"{color_channel_name}_es1"] = stdev1  # standard deviation of low res error
        basic_metrics[f"{color_channel_name}_es2"] = stdev2

    #monochrome metric is the standard deviation of hue weighted by saturation
    basic_metrics["HSV_monos"] = weighted_circular_std_deg(h, s)

    # measure the degree to which the hue is close to each of the 6 cardinal hues
    basic_metrics["HSV_h000s"] = np.mean((((h + 180 - 0) % 360) - 180)**2)**(1/2)
    basic_metrics["HSV_h060s"] = np.mean((((h + 180 - 60) % 360) - 180)**2)**(1/2)
    basic_metrics["HSV_h120s"] = np.mean((((h + 180 - 120) % 360) - 180)**2)**(1/2)
    basic_metrics["HSV_h180s"] = np.mean((((h + 180 - 180) % 360) - 180)**2)**(1/2)    
    basic_metrics["HSV_h240s"] = np.mean((((h + 180 - 240) % 360) - 180)**2)**(1/2)  
    basic_metrics["HSV_h300s"] = np.mean((((h + 180 - 300) % 360) - 180)**2)**(1/2)

    return basic_metrics

# Export metrics to CSV
def export_metrics_to_csv(frame_count_list, metrics, filename):
    """
    Export frame count and metric data to a CSV file using pandas,
    with metrics sorted alphabetically by key.

    Parameters:
    - frame_count_list (list): List of frame counts.
    - metrics (dict): Dictionary where each value is a list of the same length as frame_count_list.
    - filename (str): Name of the CSV file to write.
    """

    # Sort the metric keys alphabetically
    sorted_keys = sorted(metrics.keys())

    # Create a DataFrame using the sorted keys
    data = {'frame_count_list': frame_count_list}
    for key in sorted_keys:
        data[key] = metrics[key]

    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)

def process_video_to_midi(video_path, 
                          subdir_name, # output prefix 
                          frames_per_second, 
                          beats_per_frame,
                          ticks_per_beat, 
                          beats_per_minute, 
                          cc_number, 
                          midi_channel,
                          downscale_factor1,
                          downscale_factor2,
                          filter_width):
    """
    Process every Nth frame, calculate metrics, and generate multiple MIDI files.
    
    :param video_path: Path to the video file.
    :param output_prefix: Prefix for output MIDI filenames.
    :param frames_per_second (number of frames per second in video)
    :param beats_per_frame (number of beats between each frame that will per processed)
    :param ticks_per_beat (number of midi ticks per beat in DAW)
    :param beats_per_minute (number of beats per minute in DAW)
    :param cc_number: MIDI CC number (default 7 for volume).
    :param channel: MIDI channel (0-15).
    :param downscale_factor1: spatial scale for computing metrics
    :param downscale_factor2: resolution reduction for computing metrics
    :param filter_width: width of boxcar filter for smoothing

    """




    ticks_per_frame = ticks_per_beat *( beats_per_minute / 60.) / frames_per_second # ticks per second / frames per second
    # Calculate the frame interval for processing frames
    seconds_per_analysis_frame = beats_per_frame / (beats_per_minute / 60) # beats per frame / beats per second
    frames_per_analysis_frame_real = seconds_per_analysis_frame * frames_per_second
    # Take every Nth frame, where frames_per_interval_real is the floating point non-integer version of N

    frame_count = 0
    frame_count_list = []

    # Define metric categories that get computed by <compute_metrics>
    # and the color channels that get computed
    metric_names = ["avg", "var", "lrg", "xps", "rfl", "rad", "lin","ee1","ee2","ed1","ed2","es1","es2"]
    color_channel_names = ["R", "G", "B", "Gray","V"]
    basic_metrics = {f"{color_channel_name}_{metric_name}": [] 
               for color_channel_name in color_channel_names for metric_name in metric_names}
    # add metrics that are outside of the normal grouping
    basic_metrics["HSV_monos"] = []
    basic_metrics["HSV_h000s"] = []
    basic_metrics["HSV_h060s"] = []
    basic_metrics["HSV_h120s"] = []
    basic_metrics["HSV_h180s"] = []
    basic_metrics["HSV_h240s"] = []
    basic_metrics["HSV_h300s"] = []

    # open rhe video file
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video file.")
        return
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        k = frame_count / frames_per_analysis_frame_real
        k_rounded = round(k)
        frame_count_good = round(k_rounded * frames_per_analysis_frame_real)
        if frame_count == frame_count_good :
            print ("Processing frame:", frame_count)
            frame_count_list.append(frame_count)

            metric_results = compute_basic_metrics(frame, downscale_factor1, downscale_factor2)

            #append to the dictionary of results
            for key, value in metric_results.items():
                basic_metrics[key].append(value)

        frame_count += 1

    cap.release()

    #now compute derivative metrics that are computed after all frames are processed
    basic_metrics["HSV_monos"] = np.array(basic_metrics["HSV_monos"])
    diff_monos = np.max(basic_metrics["HSV_monos"]) - basic_metrics["HSV_monos"]

    for key in ["HSV_h000s", "HSV_h060s", "HSV_h120s", "HSV_h180s", "HSV_h240s", "HSV_h300s"]:
        basic_metrics[key] = np.array(basic_metrics[key])
        # replace trailing s in key with i      
        key_i = key.replace("s", "i") # i for intensity !
        basic_metrics[key_i] = (180 - basic_metrics[key]) * diff_monos


    # Export metrics to CSV
    csv_filename = f"{subdir_name}_basic.csv"
    export_metrics_to_csv(frame_count_list, basic_metrics, csv_filename)
    print(f"Metrics exported to {csv_filename}")

    




# Example usage
#video_file = "Mz3DllgimbrV2.wmv"  #  small test video file
#subdir_name = "Mz3DllgimbrV2" # output prefix
#video_file = "He saw Julias everywhere (MzJuliaV2e).wmv"
#video_file = "Mz3DllgimbrV2B.wmv"
#subdir_name = "Mz3DllgimbrV2B" # output prefix
#video_file = "M10zul.wmv"
#subdir_name = "M10zul" # output prefix
#video_file = "JuliaInJulia-Mzljdjb6fa2f.wmv"
#subdir_name = "JuliaInJulia" # output prefix
video_file = "MzUL2-5jm3f.wmv"
subdir_name = "MzUL2-5jm3f" # output prefix
#video_file = "WhatsApp Video 2023-09-06 at 7.38.17 AM.mp4"
#subdir_name = "WhatsApp" # output prefix

process_video_to_midi(video_file, 
                      subdir_name, # output prefix
                      frames_per_second=30, 
                      beats_per_frame=2,
                      ticks_per_beat=480, 
                      beats_per_minute=92,  
                      cc_number=1, 
                      midi_channel=0,
                      downscale_factor1=100, # scale boundary means divide so 100x100 pixels in a cell (approximately square root of width and height of video)
                      downscale_factor2=10, # resolution reduction means divide so 10x10 pixels in a cell (approximately square root of the larger scale)
                      filter_width = 5 ) # smooth the data with a triangular filter of this (odd) width
# process_video_to_midi("path_to_your_video.mp4", "output_prefix", nth_frame=30, frames_per_second=30, ticks_per_beat=480, beats_per_minute=120, cc_number=7, channel=0)


