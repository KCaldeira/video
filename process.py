# video to midi code
"""
prompt for ChatGTP:

Here's a Python script that processes every Nth frame of a video file, calculates multiple metrics (average intensity, 
standard deviation, and entropy) for each color color_channel (R, G, B, and grayscale) and outputs 24 MIDI files. 
Each metric is stored in two MIDI files: one directly scaled (0-127) and another inverted (127-0).

This script will:

Extract every Nth frame from the video.
Compute various metrics for each color channel.
Map values to MIDI Control Change (CC) messages (both direct and inverted).
Save each metric in a separate MIDI file, with filenames autogenerated.

"""
import cv2
import numpy as np
from mido import Message, MidiFile, MidiTrack
from scipy.stats import rankdata

def information_metric(color_channel, downscale_factor=4):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)
    restored = cv2.resize(downscaled, (w, h), interpolation=cv2.INTER_LINEAR)

    # Compute mean squared error (MSE) between original and restored image
    mse = np.mean((color_channel - restored) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse = 1.- mse / np.var(color_channel) 
    # 1 means all information is at coarser spatial scales, 
    # 0 means all information is at finer spatial scales

    return normalized_mse

def tranpose_metric(color_channel, downscale_factor=4):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and restored image
    mse = np.mean((downscaled - downscaled[::-1,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse = 1 - mse / np.var(downscaled) 
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

def reflect_metric(color_channel, downscale_factor=4):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and vertically reflected image
    mse0 = np.mean((downscaled - downscaled[::-1]) ** 2)

    # Compute mean squared error (MSE) between original and horizontally reflected image
    mse1 = np.mean((downscaled - downscaled[:,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse = 1 -(mse0 + mse1) / (2. * np.var(downscaled) )
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

def radial_symmetry_metric(color_channel, scale_factor):
    """
    Compute radial symmetry metric for a color channel with distance bins of width `scale_factor`.
    """
    # Step 1: Create coordinate grid
    y, x = np.indices(color_channel.shape)
    center_y = (color_channel.shape[0] / 2) - 0.5
    center_x = (color_channel.shape[1] / 2) - 0.5

    # Step 2: Compute radial distance from center for each pixel
    r = np.sqrt((x - center_x)**2 + (y - center_y)**2)

    # Step 2b: Bin distances into scale_factor-wide bins
    r_bin = (r / scale_factor).astype(np.int32)

    # Step 3: Compute mean value for each radial bin
    max_bin = r_bin.max()
    radial_mean = np.zeros(max_bin + 1)
    counts = np.bincount(r_bin.ravel())
    sums = np.bincount(r_bin.ravel(), weights=color_channel.ravel())

    # Avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        radial_mean[:len(sums)] = np.where(counts != 0, sums / counts, 0)

    # Optional: remove zeros or masked bins if they skew the variance
    # valid = counts > 0
    # return np.var(radial_mean[valid])

    return np.var(radial_mean)


def bgr_to_cmyk(b, g, r):
    """
    Convert BGR to CMYK color space.
    """
    b = b.astype(float) / 255.0
    g = g.astype(float) / 255.0
    r = r.astype(float) / 255.0

    k = 1 - np.max([b, g, r], axis=0) # 1 if black or highest color intensity, 0 if white
    c = (1 - b - k) / (1 - k + 1e-10)
    m = (1 - g - k) / (1 - k + 1e-10)
    y = (1 - r - k) / (1 - k + 1e-10)

    return c, m, y, k

def percentile_data(data):
    """
    Transform the vector <data> into a percentile list where 0 is the lowest and 1 the highest.
    """
    ranks = rankdata(data, method='average')
    percentiles = (ranks-1) / (len(data)-1)
    return percentiles


def compute_metrics(frame,scale_boundary=30):
    """
    Compute different intensity-based metrics on R, G, B, and grayscale images.
    Returns a dictionary of results.
    """
    metrics = {}
    
    b, g, r = cv2.split(frame)
    c, m, y, k = bgr_to_cmyk(b, g, r)
  
    # Convert to grayscale
    gray = 255 - cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 255 is black, 0 is white

    # Split into R, G, B channels

    for name, color_channel in [("R", r), ("G", g), ("B", b),
                                ("C", c), ("M", m), ("Y", y), ("K", k),
                                ("Gray", gray)]:
        avg_intensity = np.mean(color_channel)
        variance = np.var(color_channel)

        # 1 means all information is at finer spatial scales,
        # 0 means all information is at coarser spatial scales
        large_scale_info = information_metric(color_channel, scale_boundary) # fraction info at small and medium scales

        transpose_metric_value = tranpose_metric(color_channel, scale_boundary) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        reflect_metric_value = reflect_metric(color_channel, scale_boundary) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        radial_symmetry_metric_value = radial_symmetry_metric(color_channel, scale_boundary) # degree of symmettry for flipping around the center point
        # at the specified spatial scale

        # Store values
        metrics[f"{name}_avg"] = avg_intensity
        metrics[f"{name}_var"] = variance # note that variance is total info (i.e., diff^2 relative to mean)
        metrics[f"{name}_large"] =  large_scale_info # fraction of info at large scales
        metrics[f"{name}_transpose"] = transpose_metric_value
        metrics[f"{name}_reflect"] = reflect_metric_value
        metrics[f"{name}_radial"] = radial_symmetry_metric_value

    return metrics

def boxcar_filter_odd(data, N):
    if N < 1:
        raise ValueError("Filter length N must be at least 1.")
    if N % 2 == 0:
        raise ValueError("Filter length N should be odd for symmetric padding.")

    half_window = N // 2
    padded = np.pad(data, pad_width=half_window, mode='edge')
    kernel = np.ones(N) / N
    filtered = np.convolve(padded, kernel, mode='valid')
    return filtered

def process_video_to_midi(video_path, 
                          output_prefix, 
                          frames_per_second, 
                          beats_per_frame,
                          ticks_per_beat, 
                          beats_per_minute, 
                          cc_number, 
                          midi_channel,
                          scale_boundary,
                          filter_width):
    """
    Process every Nth frame, calculate metrics, and generate multiple MIDI files.
    
    :param video_path: Path to the video file.
    :param output_prefix: Prefix for output MIDI filenames.
    :param frames_per_second (number of frames per second in video)
    :param beats_per_frame (number of beats between each frame that will per processed)
    :param ticks_per_beat (number of midi ticks per beat in DAW)
    :param beats_per_minute (number of beats per minute in DAW)
    :param cc_number: MIDI CC number (default 7 for volume).
    :param channel: MIDI channel (0-15).
    :param scale_boundary: spatial scale for computing metrics
    :param filter_width: width of boxcar filter for smoothing

    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video file.")
        return

    # Define metric categories
    metric_names = ["avg", "var", "large", "transpose", "reflect", "radial"]
    color_channels = ["R", "G", "B", "C", "M", "Y", "K", "Gray"]

    ticks_per_frame = ticks_per_beat *( beats_per_minute / 60.) / frames_per_second # ticks per second / frames per second
    # Calculate the frame interval for processing frames
    seconds_per_analysis_frame = beats_per_frame / (beats_per_minute / 60) # beats per frame / beats per second
    frames_per_analysis_frame_real = seconds_per_analysis_frame * frames_per_second
    # Take every Nth frame, where frames_per_interval_real is the floating point non-integer version of N

    frame_count = 0
    frame_count_list = []


    # assemble dictionary of results for metrics
    metrics = {f"{color_channel}_{metric}": [] for color_channel in color_channels for metric in metric_names}
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        k = frame_count / frames_per_analysis_frame_real
        k_rounded = round(k)
        frame_count_good = round(k_rounded * frames_per_analysis_frame_real)
        if frame_count == frame_count_good :
            print ("Processing frame:", frame_count)
            frame_count_list.append(frame_count)

            metric_results = compute_metrics(frame, scale_boundary)

            #append to the dictionary of results
            for key, value in metric_results.items():
                metrics[key].append(value)

        frame_count += 1

    cap.release()

    # normalize all metrics to be between 0 and 1, with a percentile mapping
    for key, values in metrics.items():
        metric = np.array(values)
        # max_val = np.max(metric)
        # min_val = np.min(metric)
        # normalized_metric = (metric - min_val) / (max_val - min_val)
        # normalize by taking a percentile ranking
        normalized_metric = percentile_data(metric)
        metrics[key] = normalized_metric.tolist()

    # create derived metrics that involve combining metrics
    for color_channel in color_channels:
        # find minimum of "transpose", "reflect", "radial" metrics
        transpose = metrics[f"{color_channel}_transpose"]
        reflect = metrics[f"{color_channel}_reflect"]
        radial = metrics[f"{color_channel}_radial"]
        symmetry = np.minimum.reduce([transpose, reflect, radial])
        metrics[f"{color_channel}_symmetry"] = symmetry.tolist()

    # create derived metrics that involve combining color channels
    for metric in metric_names + ["symmetry"]:
        # find minimum of "transpose", "reflect", "radial" metrics
        r = metrics[f"R_{metric}"]
        g = metrics[f"G_{metric}"]
        b = metrics[f"B_{metric}"]
        c = metrics[f"C_{metric}"]
        m = metrics[f"M_{metric}"]
        y = metrics[f"Y_{metric}"]

        minRGB = np.minimum.reduce([r, g, b], axis=0)
        metrics[f"minRGB_{metric}"] = minRGB.tolist()

        minCMY = np.minimum.reduce([c, m, y], axis=0)
        metrics[f"minCMY_{metric}"] = minCMY.tolist()

    # Smooth the data with a boxcar filter
    if filter_width > 1:
        for key, values in metrics.items():
            metrics[key] = boxcar_filter_odd(np.array(values), filter_width).tolist()

    # Initialize MIDI files for each metric
    midi_files = {}
    for color_channel in color_channels + ["minRGB", "minCMY"]:
        
        for metric in metric_names +  ["symmetry"]:
            # base_filename = f"{output_prefix}_{color_channel}_{metric}"
            midi_files[f"{color_channel}_{metric}"] = MidiFile()
            midi_files[f"{color_channel}_{metric}_inv"] = MidiFile()

            # Add MIDI tracks
            midi_files[f"{color_channel}_{metric}"].tracks.append(MidiTrack())
            midi_files[f"{color_channel}_{metric}_inv"].tracks.append(MidiTrack())
    
    # Write MIDI messages for each metric
    for key, value in metrics.items():
        midi_val = [round(104 * val) for val in value] # Scale to MIDI range (0-104, avoid 105-127)
        # Invert the MIDI value for the second file
        midi_val_inv = [round(104 * (1-val)) for val in value]

        # Add to the correct MIDI track
        color_channel_name, metric_name = key.split("_")
        for i, midi_value in enumerate(midi_val):
            if i == 0:
                time_tick = 0
            else:
                time_tick = int(ticks_per_frame * (frame_count_list[i] -frame_count_list[i-1]) )
            midi_files[f"{color_channel_name}_{metric_name}"].tracks[0].append(
                Message('control_change', 
                        control=cc_number, 
                        value=midi_value, 
                        channel=midi_channel, 
                        time=time_tick)
            )
        for i, midi_value_inv in enumerate(midi_val_inv):
            if i == 0:
                time_tick = 0
            else:
                time_tick = int(ticks_per_frame * (frame_count_list[i] -frame_count_list[i-1]) )


            midi_files[f"{color_channel_name}_{metric_name}_inv"].tracks[0].append(
                Message('control_change', 
                        control=cc_number, 
                        value=midi_value_inv, 
                        channel=midi_channel, 
                        time=time_tick)
            )

    # Save all MIDI files
    for key, midi_file in midi_files.items():
        filename = f"{output_prefix}_{key}.mid"
        midi_file.save(filename)
        print(f"Saved: {filename}")

# Example usage
#test_video = "Mz3DllgimbrV2.wmv"
video_file = "He saw Julias everywhere (MzJuliaV2e).wmv"

process_video_to_midi(video_file, 
                      "Julias everywhere", 
                      frames_per_second=30, 
                      beats_per_frame=1,
                      ticks_per_beat=480, 
                      beats_per_minute=82,  
                      cc_number=7, 
                      midi_channel=0,
                      scale_boundary=6, # scale boundary means divide so 30 pixels in a cell
                      filter_width = 5 ) # smooth the data with a boxcar filter of this (odd) width
# process_video_to_midi("path_to_your_video.mp4", "output_prefix", nth_frame=30, frames_per_second=30, ticks_per_beat=480, beats_per_minute=120, cc_number=7, channel=0)

