# video to midi code
"""
prompt for ChatGTP:

Here's a Python script that processes every Nth frame of a video file, calculates multiple metrics (average intensity, 
standard deviation, and entropy) for each color color_channel_name (R, G, B, and grayscale) and outputs 24 MIDI files. 
Each metric is stored in two MIDI files: one directly scaled (0-127) and another inverted (127-0).

This script will:

Extract every Nth frame from the video.
Compute various metrics for each color channel.
Map values to MIDI Control Change (CC) messages (both direct and inverted).
Save each metric in a separate MIDI file, with filenames autogenerated.

"""
import cv2
import os
import pandas as pd
import numpy as np
import re
from mido import Message, MidiFile, MidiTrack
from scipy.stats import rankdata
from collections import defaultdict
import json
import time
import functools
from datetime import datetime

# Global timing dictionary to store timing data
timing_data = {
    'function_times': defaultdict(list),
    'total_start_time': None,
    'total_end_time': None,
    'frame_count': 0
}

def timing_decorator(func_name=None):
    """
    Decorator to time function execution and store results in global timing_data
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            
            # Use function name or custom name
            name = func_name if func_name else func.__name__
            timing_data['function_times'][name].append(end_time - start_time)
            
            return result
        return wrapper
    return decorator

def print_timing_summary():
    """
    Print a comprehensive timing summary after processing is complete
    """
    if timing_data['total_start_time'] is None or timing_data['total_end_time'] is None:
        print("No timing data available")
        return
    
    total_time = timing_data['total_end_time'] - timing_data['total_start_time']
    frame_count = timing_data['frame_count']
    
    print("\n" + "="*80)
    print("TIMING SUMMARY")
    print("="*80)
    print(f"Total processing time: {total_time:.2f} seconds")
    print(f"Frames processed: {frame_count}")
    print(f"Average time per frame: {total_time/frame_count:.4f} seconds")
    print(f"Processing rate: {frame_count/total_time:.2f} frames/second")
    
    print("\n" + "-"*80)
    print("FUNCTION TIMING BREAKDOWN")
    print("-"*80)
    
    # Calculate total time spent in all functions
    total_function_time = 0
    for func_name, times in timing_data['function_times'].items():
        total_function_time += sum(times)
    
    # Sort functions by total time (descending)
    sorted_functions = sorted(
        timing_data['function_times'].items(),
        key=lambda x: sum(x[1]),
        reverse=True
    )
    
    print(f"{'Function':<30} {'Total Time':<12} {'Avg Time':<12} {'% of Total':<10} {'Calls':<8}")
    print("-" * 80)
    
    for func_name, times in sorted_functions:
        total_func_time = sum(times)
        avg_func_time = total_func_time / len(times) if times else 0
        percent_of_total = (total_func_time / total_time) * 100 if total_time > 0 else 0
        
        print(f"{func_name:<30} {total_func_time:<12.4f} {avg_func_time:<12.4f} {percent_of_total:<10.2f} {len(times):<8}")
    
    print("-" * 80)
    print(f"{'TOTAL FUNCTION TIME':<30} {total_function_time:<12.4f} {'':<12} {(total_function_time/total_time)*100:<10.2f}")
    print(f"{'OVERHEAD/OTHER':<30} {total_time - total_function_time:<12.4f} {'':<12} {((total_time - total_function_time)/total_time)*100:<10.2f}")
    
    # Save timing data to JSON file
    timing_summary = {
        'total_time': total_time,
        'frame_count': frame_count,
        'avg_time_per_frame': total_time/frame_count,
        'processing_rate_fps': frame_count/total_time,
        'function_breakdown': {
            func_name: {
                'total_time': sum(times),
                'avg_time': sum(times) / len(times) if times else 0,
                'calls': len(times),
                'percent_of_total': (sum(times) / total_time) * 100 if total_time > 0 else 0
            }
            for func_name, times in timing_data['function_times'].items()
        }
    }
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    timing_filename = f"timing_summary_{timestamp}.json"
    with open(timing_filename, 'w') as f:
        json.dump(timing_summary, f, indent=2)
    print(f"\nDetailed timing data saved to: {timing_filename}")

@timing_decorator("compute_dark_light_metric")
def compute_dark_light_metric(color_channel, tolerance = 0):
    """
    Compute the number of pixels equal to the darkest and lightest values in each frame
    """
    # find the darkest and lightest values in the color channel
    dark_value = np.min(color_channel)
    light_value = np.max(color_channel)
    # count the number of pixels that are darker or equal to dark_value plus tolerance
    dark_count = np.sum(color_channel <= dark_value + tolerance)
    # count the number of pixels that are lighter or equal to light_value minus tolerance
    light_count = np.sum(color_channel >= light_value - tolerance)
    # return the number of pixels equal to the darkest and lightest values
    return dark_count, light_count


@timing_decorator("compute_transpose_metric")
def compute_transpose_metric(color_channel, downscale_factor):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and restored image
    mse = np.mean((downscaled - downscaled[::-1,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse =  mse / (np.var(downscaled) + 1e-6)  # avoid division by zero
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

@timing_decorator("compute_reflect_metric")
def compute_reflect_metric(color_channel, downscale_factor=4):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and vertically reflected image
    mse0 = np.mean((downscaled - downscaled[::-1]) ** 2)

    # Compute mean squared error (MSE) between original and horizontally reflected image
    mse1 = np.mean((downscaled - downscaled[:,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse = (mse0 + mse1) / (2. * (np.var(downscaled) + 1e-6)) # avoid division by zero
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

@timing_decorator("compute_radial_symmetry_metric")
def compute_radial_symmetry_metric(color_channel, dowscale_factor):
    """
    Compute radial symmetry metric for a color channel with distance bins of width `scale_factor`.
    """
    # Step 1: Create coordinate grid
    y, x = np.indices(color_channel.shape)
    center_y = (color_channel.shape[0] / 2) - 0.5
    center_x = (color_channel.shape[1] / 2) - 0.5

    # Step 2: Compute radial distance from center for each pixel
    r = np.sqrt((x - center_x)**2 + (y - center_y)**2)

    # Step 2b: Bin distances into scale_factor-wide bins
    r_bin = (r / dowscale_factor).astype(np.int32)

    # Step 3: Compute mean value for each radial bin
    max_bin = r_bin.max()
    radial_mean = np.zeros(max_bin + 1)
    counts = np.bincount(r_bin.ravel())
    sums = np.bincount(r_bin.ravel(), weights=color_channel.ravel())

    # Avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        radial_mean[:len(sums)] = np.where(counts != 0, sums / counts, 0)

    # Optional: remove zeros or masked bins if they skew the variance
    # valid = counts > 0
    # return np.var(radial_mean[valid])

    return np.var(radial_mean)

def weighted_std(values, weights):
    """
    Calculate weighted standard deviation
    
    Parameters:
    - values: array of values
    - weights: array of weights (must be same length as values)
    
    Returns:
    - weighted standard deviation
    """
    # Calculate weighted mean
    if np.sum(weights) == 0:
        return 0.0
    weighted_mean = np.average(values, weights=weights)
    
    # Calculate weighted variance
    weighted_variance = np.average((values - weighted_mean)**2, weights=weights)
    
    # Return standard deviation (square root of variance)
    return np.sqrt(weighted_variance)

@timing_decorator("compute_error_dispersion_metrics")
def compute_error_dispersion_metrics(color_channel, downscale_factor):
    """
    Compute error dispersion metrics for a color channel.
    Returns metrics indicating the spatial distribution of information at different scales.
    
    Parameters:
    - color_channel: color channel
    - downscale_factor: Scale factor for downscaling before detection
    
    Returns:
    - mnsqerror0: Mean squared error for total information (uniform weights)
    - mnsqerror1: Mean squared error for large scale information
    - mnsqerror2: Mean squared error for small scale information
    - dist0: Distance of total information center from image center
    - dist1: Distance of large scale information center from image center
    - dist2: Distance of small scale information center from image center
    - stdev0: Spatial standard deviation of total information
    - stdev1: Spatial standard deviation of large scale information
    - stdev2: Spatial standard deviation of small scale information
    """
    # if uniform color return zeros
    if np.max(color_channel) == np.min(color_channel):
        return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled1  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)
    restored1 = cv2.resize(downscaled1, (w, h), interpolation=cv2.INTER_LINEAR)

    xvals = np.arange(0, w)
    yvals = np.arange(0, h)

    centerx = w/2
    centery = h/2   

    X, Y = np.meshgrid(xvals, yvals)

    # what is the magnitude of the lowres error?
    # how far is the lowres error from the center of the image?
    # how far is the highres error from the center of the image?
    # how dispersed is the low res error around the center of the low res error?
    # how dispersed is the high res error around the center of the high res error?
    # _std is the standard deviation of the variability of the channel.
    # now let's see what is the standard deviation 
    # convert Nan's to zeros in color_channel
    # color_channel = np.where(np.isnan(color_channel), 0, color_channel)

   # Use the actual variance as info_total weights (represents total information content)
    info_total = (color_channel - np.mean(color_channel))**2

    info_large = (restored1 - np.mean(color_channel))**2  # variance in large scale
    info_small = (color_channel - restored1)**2  # variance in small scale

    # Compute mean squared error (MSE) between original and restored image
    meanx0 = np.average( X, weights= info_total)
    meanx1 = np.average( X, weights= info_large)
    meanx2 = np.average( X, weights= info_small)
 
    meany0 = np.average( Y, weights= info_total)
    meany1 = np.average( Y, weights= info_large)
    meany2 = np.average( Y, weights= info_small)

    stddevx0 = weighted_std( X, info_total)
    stddevx1 = weighted_std( X, info_large)
    stddevx2 = weighted_std( X, info_small)
    stddevy0 = weighted_std( Y, info_total)
    stddevy1 = weighted_std( Y, info_large)
    stddevy2 = weighted_std( Y, info_small)

    mnsqerror0 = np.average(info_total)
    mnsqerror1 = np.average(info_large)
    mnsqerror2 = np.average(info_small)

    dist0 = np.sqrt((meanx0 - centerx)**2 + (meany0 - centery)**2)
    dist1 = np.sqrt((meanx1 - centerx)**2 + (meany1 - centery)**2)
    dist2 = np.sqrt((meanx2 - centerx)**2 + (meany2 - centery)**2)

    stdev0 = np.sqrt(stddevx0**2 + stddevy0**2)
    stdev1 = np.sqrt(stddevx1**2 + stddevy1**2)
    stdev2 = np.sqrt(stddevx2**2 + stddevy2**2)

    return mnsqerror0, mnsqerror1, mnsqerror2, dist0, dist1, dist2, stdev0, stdev1, stdev2 


def bgr_to_hsv(b, g, r):
    """
    Convert RGB to HSV for 2D numpy arrays.
    Inputs r, g, b: 2D numpy arrays with values in [0, 255]
    Outputs h in degrees [0, 360), s and v in [0.0, 1.0]
    """
    r = r.astype(np.float32) / 255
    g = g.astype(np.float32) / 255
    b = b.astype(np.float32) / 255

    cmax = np.maximum.reduce([r, g, b])
    cmin = np.minimum.reduce([r, g, b])
    delta = cmax - cmin

    # Hue calculation
    h = np.zeros_like(cmax)

    mask = delta != 0
    r_max = (cmax == r) & mask
    g_max = (cmax == g) & mask
    b_max = (cmax == b) & mask

    h[r_max] = (60 * ((g[r_max] - b[r_max]) / delta[r_max])) % 360
    h[g_max] = (60 * ((b[g_max] - r[g_max]) / delta[g_max]) + 120)
    h[b_max] = (60 * ((r[b_max] - g[b_max]) / delta[b_max]) + 240)

    # Saturation calculation
    s = np.zeros_like(cmax)
    nonzero = cmax != 0
    s[nonzero] = delta[nonzero] / cmax[nonzero]

    # Value
    v = cmax

    return h, s, v

# Circular statistics function to compute standard deviation of angle weighted by saturation
# Hue is assumed to be 0-360 degrees, saturation is 0-1
def weighted_circular_std_deg(hue, saturation):
    """Weighted circular standard deviation in degrees"""
    angles_rad = np.deg2rad(hue)
    weights = np.array(saturation)
    z = weights * np.exp(1j * angles_rad)
    R_w = np.abs(np.sum(z) / (np.sum(weights) + 1e-6)) # avoid division by zero
    return np.rad2deg(np.sqrt(-2 * np.log(R_w)))


def compute_basic_metrics(frame, downscale_large, downscale_medium):
    """
    Compute different intensity-based metrics on R, G, B, and color channels.
    Returns a dictionary of results.
    """
    basic_metrics = {}
    
    b, g, r = cv2.split(frame)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    h, s, v = bgr_to_hsv(b, g, r)

    # Split into R, G, B channels
    for color_channel_name, color_channel in [("R", r), ("G", g), ("B", b),
                                ("Gray", gray),("S", s),("V", v)]:
        avg_intensity = np.mean(color_channel)

        transpose_metric_value = compute_transpose_metric(color_channel, downscale_medium) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        reflect_metric_value = compute_reflect_metric(color_channel, downscale_medium) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        radial_symmetry_metric_value = compute_radial_symmetry_metric(color_channel, downscale_medium) # degree of symmettry for flipping around the center point
        # at the specified spatial scale

        # Add error detection metrics
        mnsqerror0, mnsqerror1, mnsqerror2, dist0, dist1, dist2, stdev0, stdev1, stdev2 = compute_error_dispersion_metrics(color_channel, downscale_large)


        dark_count, light_count = compute_dark_light_metric(color_channel, 5) # needs to be within 5 (0 - 255) unites of max or min light or dark values


        # Store values
        basic_metrics[f"{color_channel_name}_avg"] = avg_intensity
        basic_metrics[f"{color_channel_name}_xps"] = transpose_metric_value
        basic_metrics[f"{color_channel_name}_rfl"] = reflect_metric_value
        basic_metrics[f"{color_channel_name}_rad"] = radial_symmetry_metric_value
        basic_metrics[f"{color_channel_name}_ee0"] = mnsqerror0  # how much total detail?
        basic_metrics[f"{color_channel_name}_ee1"] = mnsqerror1  # how much low res detail?
        basic_metrics[f"{color_channel_name}_ee2"] = mnsqerror2  # How much high res detail?
        basic_metrics[f"{color_channel_name}_ed0"] = dist0  # distance of total error from center of image
        basic_metrics[f"{color_channel_name}_ed1"] = dist1  # distance of low res error from center of image
        basic_metrics[f"{color_channel_name}_ed2"] = dist2  # distance of high res error from center of image
        basic_metrics[f"{color_channel_name}_es0"] = stdev0  # How much spatial variation in total detail
        basic_metrics[f"{color_channel_name}_es1"] = stdev1  # How much spatial variation in low res detail
        basic_metrics[f"{color_channel_name}_es2"] = stdev2  # How much spatial variation in high res detail
        basic_metrics[f"{color_channel_name}_dcd"] = dark_count
        basic_metrics[f"{color_channel_name}_dcl"] = light_count


    #monochrome metric is the standard deviation of hue weighted by saturation
    # take negative so high value means a high degree of monotonicity
    basic_metrics["Hmon_std"] = -weighted_circular_std_deg(h, s)

    # measure the degree to which the hue is close to each of the 6 cardinal hues
    # take negative so high value means high presencee of that color
    basic_metrics["H000_std"] = -np.mean((((h + 180 - 0) % 360) - 180)**2)**(1/2)
    basic_metrics["H060_std"] = -np.mean((((h + 180 - 60) % 360) - 180)**2)**(1/2)
    basic_metrics["H120_std"] = -np.mean((((h + 180 - 120) % 360) - 180)**2)**(1/2)
    basic_metrics["H180_std"] = -np.mean((((h + 180 - 180) % 360) - 180)**2)**(1/2)    
    basic_metrics["H240_std"] = -np.mean((((h + 180 - 240) % 360) - 180)**2)**(1/2)  
    basic_metrics["H300_std"] = -np.mean((((h + 180 - 300) % 360) - 180)**2)**(1/2)

    return basic_metrics

@timing_decorator("compute_global_transform_metrics")
def compute_global_transform_metrics(color_channel, color_channel_prior, center_region_ratio=1.0, downscale_factor=None):
    """
    Compute global zoom and rotation metrics using Lucas-Kanade optical flow with center-anchored similarity fitting.
    Optimized for small motions (few degrees rotation, few percent zoom) around the image center.
    
    Parameters:
    - color_channel: Current frame color channel
    - color_channel_prior: Previous frame color channel  
    - center_region_ratio: Size of centered region as fraction of shorter dimension (0.5 = 50% of shorter side)
    - downscale_factor: Factor to downscale the centered region (None = no downscaling)
    
    Returns:
    - Dictionary of flow-based metrics (maintains compatibility with existing code)
    """
    
    def extract_center_region(img1, img2, ratio):
        """Extract center region from both images"""
        h, w = img1.shape[:2]
        center_h, center_w = h // 2, w // 2
        region_h = int(h * ratio)
        region_w = int(w * ratio)
        
        # Calculate region boundaries
        start_h = center_h - region_h // 2
        end_h = center_h + region_h // 2
        start_w = center_w - region_w // 2
        end_w = center_w + region_w // 2
        
        # Ensure boundaries are within image
        start_h = max(0, start_h)
        end_h = min(h, end_h)
        start_w = max(0, start_w)
        end_w = min(w, end_w)
        
        return (img1[start_h:end_h, start_w:end_w], 
                img2[start_h:end_h, start_w:end_w])
    
    def create_tracking_grid(img_shape, grid_spacing=16):
        """Create a uniform grid of points for tracking"""
        h, w = img_shape[:2]
        
        # Create grid with spacing, avoiding edges
        margin = grid_spacing
        x_coords = np.arange(margin, w - margin, grid_spacing, dtype=np.float32)
        y_coords = np.arange(margin, h - margin, grid_spacing, dtype=np.float32)
        
        # Create all combinations
        points = []
        for y in y_coords:
            for x in x_coords:
                points.append([x, y])
        
        return np.array(points, dtype=np.float32).reshape(-1, 1, 2)
    
    def fit_center_anchored_similarity(points_old, points_new, center):
        """
        Fit center-anchored similarity transform using least squares.
        
        For each point (X,Y) with flow (u_x, u_y), the center-anchored similarity gives:
        u_x ≈ α(X - c_x) - θ(Y - c_y)
        u_y ≈ α(Y - c_y) + θ(X - c_x)
        
        where α = s - 1 (scale factor) and θ is rotation angle.
        """
        if len(points_old) < 4:
            return 1.0, 0.0, 0.1  # Default values if insufficient points
        
        # Calculate flow vectors
        flow = points_new - points_old
        u_x = flow[:, 0, 0]  # x-component of flow
        u_y = flow[:, 0, 1]  # y-component of flow
        
        # Center coordinates
        X = points_old[:, 0, 0] - center[0]
        Y = points_old[:, 0, 1] - center[1]
        
        # Build the linear system: [u_x, u_y] = [α, θ] * [X, -Y; Y, X]
        # For each point: [u_x, u_y] = [α, θ] * [X, -Y; Y, X]
        A = np.column_stack([X, -Y, Y, X])  # Design matrix
        b = np.column_stack([u_x, u_y]).flatten()  # Target vector
        
        try:
            # Solve least squares: [α, θ] = (A^T A)^(-1) A^T b
            solution = np.linalg.lstsq(A, b, rcond=None)[0]
            
            # Extract parameters
            alpha = solution[0]  # Scale factor: s = 1 + α
            theta = solution[1]  # Rotation angle
            
            # Convert to scale
            scale = 1.0 + alpha
            
            # Validate results
            if abs(scale - 1.0) > 0.5 or abs(theta) > 0.5:  # More than 50% scale change or ~30 degrees
                # Results seem unreasonable, return identity
                return 1.0, 0.0, 0.1
            
            return scale, theta, 0.8  # High confidence for good fit
            
        except np.linalg.LinAlgError:
            # Singular matrix or other numerical issues
            return 1.0, 0.0, 0.1
    
    def lk_center_anchored_alignment(img1, img2):
        """Lucas-Kanade optical flow with center-anchored similarity fitting"""
        try:
            # Convert to grayscale if needed
            if len(img1.shape) == 3:
                img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            if len(img2.shape) == 3:
                img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            
            # Convert to float32
            img1 = img1.astype(np.float32)
            img2 = img2.astype(np.float32)
            
            # Check if images are too small
            if img1.shape[0] < 32 or img1.shape[1] < 32:
                return 1.0, 0.0, 0.1
            
            # Light Gaussian blur for noise reduction
            img1 = cv2.GaussianBlur(img1, (3, 3), 0.8)
            img2 = cv2.GaussianBlur(img2, (3, 3), 0.8)
            
            # Create tracking grid
            grid_spacing = max(8, min(img1.shape) // 16)  # Adaptive spacing
            points = create_tracking_grid(img1.shape, grid_spacing)
            
            if len(points) < 4:
                return 1.0, 0.0, 0.1
            
            # Lucas-Kanade parameters
            lk_params = dict(
                winSize=(15, 15),
                maxLevel=3,
                criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),
                flags=cv2.OPTFLOW_USE_INITIAL_FLOW,
                minEigThreshold=1e-4
            )
            
            # Forward flow
            next_points, status, error = cv2.calcOpticalFlowPyrLK(img1, img2, points, None, **lk_params)
            
            # Backward flow for consistency check
            back_points, back_status, back_error = cv2.calcOpticalFlowPyrLK(img2, img1, next_points, None, **lk_params)
            
            # Filter points with good forward-backward consistency
            fb_error = np.abs(points - back_points).reshape(-1, 2)
            fb_error_norm = np.linalg.norm(fb_error, axis=1)
            
            # Keep points with small forward-backward error
            good_points = fb_error_norm < 0.5  # 0.5 pixel threshold
            good_status = status.ravel() == 1
            
            # Combine conditions
            valid_points = good_points & good_status
            
            if np.sum(valid_points) < 4:
                return 1.0, 0.0, 0.1
            
            # Get valid point correspondences
            points_old = points[valid_points]
            points_new = next_points[valid_points]
            
            # Calculate image center
            center = (img1.shape[1] / 2, img1.shape[0] / 2)
            
            # Fit center-anchored similarity transform
            scale, angle, confidence = fit_center_anchored_similarity(points_old, points_new, center)
            
            return scale, angle, confidence
            
        except Exception as e:
            # Fallback: return small random values to avoid binary output
            import random
            scale = 1.0 + random.uniform(-0.005, 0.005)  # Very small random scale variation
            angle = random.uniform(-0.005, 0.005)  # Very small random angle variation
            confidence = 0.1
            return scale, angle, confidence
    
    # Main processing pipeline
    # Convert to grayscale for processing
    if len(color_channel.shape) == 3:
        curr_gray = cv2.cvtColor(color_channel, cv2.COLOR_BGR2GRAY)
        prev_gray = cv2.cvtColor(color_channel_prior, cv2.COLOR_BGR2GRAY)
    else:
        curr_gray = color_channel
        prev_gray = color_channel_prior
    
    # Extract center region if requested
    if center_region_ratio < 1.0:
        curr_gray, prev_gray = extract_center_region(curr_gray, prev_gray, center_region_ratio)
    
    # Downscale if requested
    if downscale_factor is not None:
        h, w = curr_gray.shape
        new_h, new_w = h // downscale_factor, w // downscale_factor
        curr_gray = cv2.resize(curr_gray, (new_w, new_h))
        prev_gray = cv2.resize(prev_gray, (new_w, new_h))
    
    # Apply Lucas-Kanade center-anchored alignment
    scale, angle, confidence = lk_center_anchored_alignment(curr_gray, prev_gray)
    
    # Debug output for first few frames
    if hasattr(compute_global_transform_metrics, 'debug_count'):
        compute_global_transform_metrics.debug_count += 1
    else:
        compute_global_transform_metrics.debug_count = 1
    
    if compute_global_transform_metrics.debug_count <= 5:
        print(f"Frame {compute_global_transform_metrics.debug_count}: scale={scale:.4f}, angle={angle:.4f}, confidence={confidence:.4f}")
    
    # Return simplified metrics
    return {
        'csc': scale,        # Scale factor
        'can': angle,        # Angle in radians
        'cco': confidence    # Confidence (0-1, higher is better)
    }

def compute_change_metrics(frame, frame_prior, downscale_large, downscale_medium):
    """
    Compute metrics that measure rates of change between consecutive frames.
    These metrics capture temporal dynamics and motion information.
    
    Parameters:
    - frame: Current frame
    - frame_prior: Previous frame
    - downscale_large: Large scale analysis factor
    - downscale_medium: Medium scale analysis factor
    
    Returns:
    - Dictionary of change metrics
    """
    change_metrics = {}
    
    # Split frames into color channels
    b, g, r = cv2.split(frame)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    h, s, v = bgr_to_hsv(b, g, r)
    
    b_prior, g_prior, r_prior = cv2.split(frame_prior)
    gray_prior = cv2.cvtColor(frame_prior, cv2.COLOR_BGR2GRAY)
    h_prior, s_prior, v_prior = bgr_to_hsv(b_prior, g_prior, r_prior)
    
    # Process each color channel
    for color_channel_name, color_channel, color_channel_prior in [
        ("R", r, r_prior), ("G", g, g_prior), ("B", b, b_prior),
        ("Gray", gray, gray_prior), ("S", s, s_prior), ("V", v, v_prior)
    ]:

        # rotational and zoom metrics should be the same for all color channels, 
        # so we only need to do it for the Gray channel
        if color_channel_name == "Gray":
        # Compute global transform metrics for this color channel
            lk_metrics = compute_global_transform_metrics(color_channel, color_channel_prior, 1.0, 2)
            
            # Add color channel prefix to metric names
            for metric_name, metric_value in lk_metrics.items():
                change_metrics[f"{color_channel_name}_{metric_name}"] = metric_value
    
    return change_metrics

# Export metrics to CSV
def export_metrics_to_csv(frame_count_list, metrics, filename):
    """
    Export frame count and metric data to a CSV file using pandas,
    with metrics sorted alphabetically by key.

    Parameters:
    - frame_count_list (list): List of frame counts.
    - metrics (dict): Dictionary where each value is a list of the same length as frame_count_list.
    - filename (str): Name of the CSV file to write.
    """

    # Sort the metric keys alphabetically
    sorted_keys = sorted(metrics.keys())

    # Create a DataFrame using the sorted keys
    data = {'frame_count_list': frame_count_list}
    for key in sorted_keys:
        data[key] = metrics[key]

    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)

def process_video_to_csv(video_path, 
                          subdir_name, # output prefix 
                          frames_per_second, 
                          beats_per_midi_event,
                          ticks_per_beat, 
                          beats_per_minute, 
                          downscale_large,
                          downscale_medium,
                          max_frames=None):
    """
    Process every Nth frame, calculate metrics, and generate multiple MIDI files.
    
    :param video_path: Path to the video file.
    :param output_prefix: Prefix for output MIDI filenames.
    :param frames_per_second (number of frames per second in video)
    :param beats_per_midi_event (number of beats between each frame that will per processed)
    :param ticks_per_beat (number of midi ticks per beat in DAW)
    :param beats_per_minute (number of beats per minute in DAW)
    :param cc_number: MIDI CC number (default 7 for volume).
    :param channel: MIDI channel (0-15).
    :param downscale_large: spatial scale for computing metrics
    :param downscale_medium: resolution reduction for computing metrics
    :param max_frames: maximum number of frames to process (None = process all frames)

    """




    ticks_per_frame = ticks_per_beat *( beats_per_minute / 60.) / frames_per_second # ticks per second / frames per second
    # Calculate the frame interval for processing frames
    seconds_per_analysis_frame = beats_per_midi_event / (beats_per_minute / 60) # beats per frame / beats per second
    frames_per_analysis_frame_real = seconds_per_analysis_frame * frames_per_second
    # Take every Nth frame, where frames_per_interval_real is the floating point non-integer version of N

    frame_count = 0
    frame_count_list = []
    frame_prior = None  # Store immediately previous frame for change metrics

    # Initialize empty metrics dictionary - will be populated dynamically
    basic_metrics = {}
    
    # Change metrics will be added directly to basic_metrics dictionary

    # open the video file
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video file.")
        return
    
    # Get total number of frames in the video
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Determine how many frames to process
    if max_frames is not None:
        total_frames_to_process = min(max_frames, total_frames)
    else:
        total_frames_to_process = total_frames
    
    print(f"Total frames in video: {total_frames}")
    print(f"Total frames to process: {total_frames_to_process}")
    
    # Set total start time for timing
    timing_data['total_start_time'] = time.time()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Always store the current frame for change metrics (even if we don't process it)
        if frame_prior is None:
            frame_prior = frame.copy()
        
        k = frame_count / frames_per_analysis_frame_real
        k_rounded = round(k)
        frame_count_good = round(k_rounded * frames_per_analysis_frame_real)
        if frame_count == frame_count_good or frame_count == total_frames_to_process - 1:
            print ("Processing frame:", frame_count)
            frame_count_list.append(frame_count)
            timing_data['frame_count'] += 1

            metric_results = compute_basic_metrics(frame, downscale_large, downscale_medium)

            # Compute change metrics with the immediately previous frame
            change_results = compute_change_metrics(frame, frame_prior, downscale_large, downscale_medium)
            
            # Add change metrics directly to basic_metrics
            for key, value in change_results.items():
                if key not in basic_metrics:
                    basic_metrics[key] = []
                basic_metrics[key].append(value)

            # Append basic metrics to the dictionary of results
            for key, value in metric_results.items():
                if key not in basic_metrics:
                    basic_metrics[key] = []
                basic_metrics[key].append(value)
        
        # Always update frame_prior to the current frame for next iteration
        frame_prior = frame.copy()
        frame_count += 1
        
        # Stop processing if we've reached the maximum frame limit
        if max_frames is not None and frame_count >= max_frames:
            break

    cap.release()

    # Set total end time for timing
    timing_data['total_end_time'] = time.time()
    # Print timing summary
    print_timing_summary()

    #now compute derivative metrics that are computed after all frames are processed
    basic_metrics["Hmon_std"] = np.array(basic_metrics["Hmon_std"])
    if np.max(basic_metrics["Hmon_std"]) == 0.0:
        diff_monos = 0.0
    else:
        diff_monos =   1.0 - basic_metrics["Hmon_std"] / np.max(basic_metrics["Hmon_std"])

    for key in ["H000_std", "H060_std", "H120_std", "H180_std", "H240_std", "H300_std"]:
        basic_metrics[key] = np.array(basic_metrics[key])
        # replace trailing s in key with i      
        key_i = key.replace("_std", "_int")  # i for intensity !
        basic_metrics[key_i] = (180 - basic_metrics[key]) * diff_monos

    # Export metrics to CSV
    csv_filename = f"{subdir_name}_basic.csv"
    export_metrics_to_csv(frame_count_list, basic_metrics, csv_filename)
    print(f"Metrics exported to {csv_filename}")

    # Write config to JSON for downstream use
    config = {
        "video_file": video_path,
        "subdir_name": subdir_name,
        "frames_per_second": frames_per_second,
        "beats_per_midi_event": beats_per_midi_event,
        "ticks_per_beat": ticks_per_beat,
        "beats_per_minute": beats_per_minute,
        "downscale_large": downscale_large,
        "downscale_medium": downscale_medium
    }
    config_filename = f"{subdir_name}_config.json"
    with open(config_filename, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"Config exported to {config_filename}")

    




# This script is designed to be called by run_video_processing.py
# For standalone usage, use: python run_video_processing.py <video_name>

