# video to midi code
"""
prompt for ChatGTP:

Here's a Python script that processes every Nth frame of a video file, calculates multiple metrics (average intensity, 
standard deviation, and entropy) for each color color_channel_name (R, G, B, and grayscale) and outputs 24 MIDI files. 
Each metric is stored in two MIDI files: one directly scaled (0-127) and another inverted (127-0).

This script will:

Extract every Nth frame from the video.
Compute various metrics for each color channel.
Map values to MIDI Control Change (CC) messages (both direct and inverted).
Save each metric in a separate MIDI file, with filenames autogenerated.

"""
import cv2
import os
import pandas as pd
import numpy as np
import re
from mido import Message, MidiFile, MidiTrack
from scipy.stats import rankdata
from collections import defaultdict
import json
import time
import functools
from datetime import datetime

# Global timing dictionary to store timing data
timing_data = {
    'function_times': defaultdict(list),
    'total_start_time': None,
    'total_end_time': None,
    'frame_count': 0
}

def timing_decorator(func_name=None):
    """
    Decorator to time function execution and store results in global timing_data
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            
            # Use function name or custom name
            name = func_name if func_name else func.__name__
            timing_data['function_times'][name].append(end_time - start_time)
            
            return result
        return wrapper
    return decorator

def print_timing_summary():
    """
    Print a comprehensive timing summary after processing is complete
    """
    if timing_data['total_start_time'] is None or timing_data['total_end_time'] is None:
        print("No timing data available")
        return
    
    total_time = timing_data['total_end_time'] - timing_data['total_start_time']
    frame_count = timing_data['frame_count']
    
    print("\n" + "="*80)
    print("TIMING SUMMARY")
    print("="*80)
    print(f"Total processing time: {total_time:.2f} seconds")
    print(f"Frames processed: {frame_count}")
    print(f"Average time per frame: {total_time/frame_count:.4f} seconds")
    print(f"Processing rate: {frame_count/total_time:.2f} frames/second")
    
    print("\n" + "-"*80)
    print("FUNCTION TIMING BREAKDOWN")
    print("-"*80)
    
    # Calculate total time spent in all functions
    total_function_time = 0
    for func_name, times in timing_data['function_times'].items():
        total_function_time += sum(times)
    
    # Sort functions by total time (descending)
    sorted_functions = sorted(
        timing_data['function_times'].items(),
        key=lambda x: sum(x[1]),
        reverse=True
    )
    
    print(f"{'Function':<30} {'Total Time':<12} {'Avg Time':<12} {'% of Total':<10} {'Calls':<8}")
    print("-" * 80)
    
    for func_name, times in sorted_functions:
        total_func_time = sum(times)
        avg_func_time = total_func_time / len(times) if times else 0
        percent_of_total = (total_func_time / total_time) * 100 if total_time > 0 else 0
        
        print(f"{func_name:<30} {total_func_time:<12.4f} {avg_func_time:<12.4f} {percent_of_total:<10.2f} {len(times):<8}")
    
    print("-" * 80)
    print(f"{'TOTAL FUNCTION TIME':<30} {total_function_time:<12.4f} {'':<12} {(total_function_time/total_time)*100:<10.2f}")
    print(f"{'OVERHEAD/OTHER':<30} {total_time - total_function_time:<12.4f} {'':<12} {((total_time - total_function_time)/total_time)*100:<10.2f}")
    
    # Save timing data to JSON file
    timing_summary = {
        'total_time': total_time,
        'frame_count': frame_count,
        'avg_time_per_frame': total_time/frame_count,
        'processing_rate_fps': frame_count/total_time,
        'function_breakdown': {
            func_name: {
                'total_time': sum(times),
                'avg_time': sum(times) / len(times) if times else 0,
                'calls': len(times),
                'percent_of_total': (sum(times) / total_time) * 100 if total_time > 0 else 0
            }
            for func_name, times in timing_data['function_times'].items()
        }
    }
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    timing_filename = f"timing_summary_{timestamp}.json"
    with open(timing_filename, 'w') as f:
        json.dump(timing_summary, f, indent=2)
    print(f"\nDetailed timing data saved to: {timing_filename}")

@timing_decorator("compute_dark_light_metric")
def compute_dark_light_metric(color_channel, tolerance = 0):
    """
    Compute the number of pixels equal to the darkest and lightest values in each frame
    """
    # find the darkest and lightest values in the color channel
    dark_value = np.min(color_channel)
    light_value = np.max(color_channel)
    # count the number of pixels that are darker or equal to dark_value plus tolerance
    dark_count = np.sum(color_channel <= dark_value + tolerance)
    # count the number of pixels that are lighter or equal to light_value minus tolerance
    light_count = np.sum(color_channel >= light_value - tolerance)
    # return the number of pixels equal to the darkest and lightest values
    return dark_count, light_count


@timing_decorator("compute_transpose_metric")
def compute_transpose_metric(color_channel, downscale_factor):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and restored image
    mse = np.mean((downscaled - downscaled[::-1,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse =  mse / (np.var(downscaled) + 1e-6)  # avoid division by zero
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

@timing_decorator("compute_reflect_metric")
def compute_reflect_metric(color_channel, downscale_factor=4):
    """
    Returns a metric of information loss when a color channel from a frame 
    is downscaled and then upscaled.
    
    frame: color channel (e.g., R, G, B, or grayscale)
    downscale_factor: how much to shrink (e.g., 4 means 1/4 size)
    """

    # Original size
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)

    # Compute mean squared error (MSE) between original and vertically reflected image
    mse0 = np.mean((downscaled - downscaled[::-1]) ** 2)

    # Compute mean squared error (MSE) between original and horizontally reflected image
    mse1 = np.mean((downscaled - downscaled[:,::-1]) ** 2)

    # Optional: normalize MSE to 0–1 by dividing by max possible value (variance)
    normalized_mse = (mse0 + mse1) / (2. * (np.var(downscaled) + 1e-6)) # avoid division by zero
    # 1 means perfect symmetry at this scale 
    # 0 means no symmetry at this scale

    return normalized_mse

@timing_decorator("compute_radial_symmetry_metric")
def compute_radial_symmetry_metric(color_channel, dowscale_factor):
    """
    Compute radial symmetry metric for a color channel with distance bins of width `scale_factor`.
    """
    # Step 1: Create coordinate grid
    y, x = np.indices(color_channel.shape)
    center_y = (color_channel.shape[0] / 2) - 0.5
    center_x = (color_channel.shape[1] / 2) - 0.5

    # Step 2: Compute radial distance from center for each pixel
    r = np.sqrt((x - center_x)**2 + (y - center_y)**2)

    # Step 2b: Bin distances into scale_factor-wide bins
    r_bin = (r / dowscale_factor).astype(np.int32)

    # Step 3: Compute mean value for each radial bin
    max_bin = r_bin.max()
    radial_mean = np.zeros(max_bin + 1)
    counts = np.bincount(r_bin.ravel())
    sums = np.bincount(r_bin.ravel(), weights=color_channel.ravel())

    # Avoid division by zero
    with np.errstate(divide='ignore', invalid='ignore'):
        radial_mean[:len(sums)] = np.where(counts != 0, sums / counts, 0)

    # Optional: remove zeros or masked bins if they skew the variance
    # valid = counts > 0
    # return np.var(radial_mean[valid])

    return np.var(radial_mean)

def weighted_std(values, weights):
    """
    Calculate weighted standard deviation
    
    Parameters:
    - values: array of values
    - weights: array of weights (must be same length as values)
    
    Returns:
    - weighted standard deviation
    """
    # Calculate weighted mean
    if np.sum(weights) == 0:
        return 0.0
    weighted_mean = np.average(values, weights=weights)
    
    # Calculate weighted variance
    weighted_variance = np.average((values - weighted_mean)**2, weights=weights)
    
    # Return standard deviation (square root of variance)
    return np.sqrt(weighted_variance)

@timing_decorator("compute_error_dispersion_metrics")
def compute_error_dispersion_metrics(color_channel, downscale_factor):
    """
    Compute error dispersion metrics for a color channel.
    Returns metrics indicating the spatial distribution of information at different scales.
    
    Parameters:
    - color_channel: color channel
    - downscale_factor: Scale factor for downscaling before detection
    
    Returns:
    - mnsqerror0: Mean squared error for total information (uniform weights)
    - mnsqerror1: Mean squared error for large scale information
    - mnsqerror2: Mean squared error for small scale information
    - dist0: Distance of total information center from image center
    - dist1: Distance of large scale information center from image center
    - dist2: Distance of small scale information center from image center
    - stdev0: Spatial standard deviation of total information
    - stdev1: Spatial standard deviation of large scale information
    - stdev2: Spatial standard deviation of small scale information
    """
    # if uniform color return zeros
    if np.max(color_channel) == np.min(color_channel):
        return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
    h, w = color_channel.shape[0:2]

    # Downscale and then upscale
    downscaled1  = cv2.resize(color_channel, (w // downscale_factor, h // downscale_factor), interpolation=cv2.INTER_AREA)
    restored1 = cv2.resize(downscaled1, (w, h), interpolation=cv2.INTER_LINEAR)

    xvals = np.arange(0, w)
    yvals = np.arange(0, h)

    centerx = w/2
    centery = h/2   

    X, Y = np.meshgrid(xvals, yvals)

    # what is the magnitude of the lowres error?
    # how far is the lowres error from the center of the image?
    # how far is the highres error from the center of the image?
    # how dispersed is the low res error around the center of the low res error?
    # how dispersed is the high res error around the center of the high res error?
    # _std is the standard deviation of the variability of the channel.
    # now let's see what is the standard deviation 
    # convert Nan's to zeros in color_channel
    # color_channel = np.where(np.isnan(color_channel), 0, color_channel)

   # Use the actual variance as info_total weights (represents total information content)
    info_total = (color_channel - np.mean(color_channel))**2

    info_large = (restored1 - np.mean(color_channel))**2  # variance in large scale
    info_small = (color_channel - restored1)**2  # variance in small scale

    # Compute mean squared error (MSE) between original and restored image
    meanx0 = np.average( X, weights= info_total)
    meanx1 = np.average( X, weights= info_large)
    meanx2 = np.average( X, weights= info_small)
 
    meany0 = np.average( Y, weights= info_total)
    meany1 = np.average( Y, weights= info_large)
    meany2 = np.average( Y, weights= info_small)

    stddevx0 = weighted_std( X, info_total)
    stddevx1 = weighted_std( X, info_large)
    stddevx2 = weighted_std( X, info_small)
    stddevy0 = weighted_std( Y, info_total)
    stddevy1 = weighted_std( Y, info_large)
    stddevy2 = weighted_std( Y, info_small)

    mnsqerror0 = np.average(info_total)
    mnsqerror1 = np.average(info_large)
    mnsqerror2 = np.average(info_small)

    dist0 = np.sqrt((meanx0 - centerx)**2 + (meany0 - centery)**2)
    dist1 = np.sqrt((meanx1 - centerx)**2 + (meany1 - centery)**2)
    dist2 = np.sqrt((meanx2 - centerx)**2 + (meany2 - centery)**2)

    stdev0 = np.sqrt(stddevx0**2 + stddevy0**2)
    stdev1 = np.sqrt(stddevx1**2 + stddevy1**2)
    stdev2 = np.sqrt(stddevx2**2 + stddevy2**2)

    return mnsqerror0, mnsqerror1, mnsqerror2, dist0, dist1, dist2, stdev0, stdev1, stdev2 


def bgr_to_hsv(b, g, r):
    """
    Convert RGB to HSV for 2D numpy arrays.
    Inputs r, g, b: 2D numpy arrays with values in [0, 255]
    Outputs h in degrees [0, 360), s and v in [0.0, 1.0]
    """
    r = r.astype(np.float32) / 255
    g = g.astype(np.float32) / 255
    b = b.astype(np.float32) / 255

    cmax = np.maximum.reduce([r, g, b])
    cmin = np.minimum.reduce([r, g, b])
    delta = cmax - cmin

    # Hue calculation
    h = np.zeros_like(cmax)

    mask = delta != 0
    r_max = (cmax == r) & mask
    g_max = (cmax == g) & mask
    b_max = (cmax == b) & mask

    h[r_max] = (60 * ((g[r_max] - b[r_max]) / delta[r_max])) % 360
    h[g_max] = (60 * ((b[g_max] - r[g_max]) / delta[g_max]) + 120)
    h[b_max] = (60 * ((r[b_max] - g[b_max]) / delta[b_max]) + 240)

    # Saturation calculation
    s = np.zeros_like(cmax)
    nonzero = cmax != 0
    s[nonzero] = delta[nonzero] / cmax[nonzero]

    # Value
    v = cmax

    return h, s, v

# Circular statistics function to compute standard deviation of angle weighted by saturation
# Hue is assumed to be 0-360 degrees, saturation is 0-1
def weighted_circular_std_deg(hue, saturation):
    """Weighted circular standard deviation in degrees"""
    angles_rad = np.deg2rad(hue)
    weights = np.array(saturation)
    z = weights * np.exp(1j * angles_rad)
    R_w = np.abs(np.sum(z) / (np.sum(weights) + 1e-6)) # avoid division by zero
    return np.rad2deg(np.sqrt(-2 * np.log(R_w)))


def compute_basic_metrics(frame, downscale_large, downscale_medium):
    """
    Compute different intensity-based metrics on R, G, B, and color channels.
    Returns a dictionary of results.
    """
    basic_metrics = {}
    
    b, g, r = cv2.split(frame)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    h, s, v = bgr_to_hsv(b, g, r)

    # Split into R, G, B channels
    for color_channel_name, color_channel in [("R", r), ("G", g), ("B", b),
                                ("Gray", gray),("S", s),("V", v)]:
        avg_intensity = np.mean(color_channel)

        transpose_metric_value = compute_transpose_metric(color_channel, downscale_medium) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        reflect_metric_value = compute_reflect_metric(color_channel, downscale_medium) # degree of symmettry for flipping around the center point
        # at the specified spatial scale
        radial_symmetry_metric_value = compute_radial_symmetry_metric(color_channel, downscale_medium) # degree of symmettry for flipping around the center point
        # at the specified spatial scale

        # Add error detection metrics
        mnsqerror0, mnsqerror1, mnsqerror2, dist0, dist1, dist2, stdev0, stdev1, stdev2 = compute_error_dispersion_metrics(color_channel, downscale_large)


        dark_count, light_count = compute_dark_light_metric(color_channel, 5) # needs to be within 5 (0 - 255) unites of max or min light or dark values


        # Store values
        basic_metrics[f"{color_channel_name}_avg"] = avg_intensity
        basic_metrics[f"{color_channel_name}_xps"] = transpose_metric_value
        basic_metrics[f"{color_channel_name}_rfl"] = reflect_metric_value
        basic_metrics[f"{color_channel_name}_rad"] = radial_symmetry_metric_value
        basic_metrics[f"{color_channel_name}_ee0"] = mnsqerror0  # how much total detail?
        basic_metrics[f"{color_channel_name}_ee1"] = mnsqerror1  # how much low res detail?
        basic_metrics[f"{color_channel_name}_ee2"] = mnsqerror2  # How much high res detail?
        basic_metrics[f"{color_channel_name}_ed0"] = dist0  # distance of total error from center of image
        basic_metrics[f"{color_channel_name}_ed1"] = dist1  # distance of low res error from center of image
        basic_metrics[f"{color_channel_name}_ed2"] = dist2  # distance of high res error from center of image
        basic_metrics[f"{color_channel_name}_es0"] = stdev0  # How much spatial variation in total detail
        basic_metrics[f"{color_channel_name}_es1"] = stdev1  # How much spatial variation in low res detail
        basic_metrics[f"{color_channel_name}_es2"] = stdev2  # How much spatial variation in high res detail
        basic_metrics[f"{color_channel_name}_dcd"] = dark_count
        basic_metrics[f"{color_channel_name}_dcl"] = light_count


    #monochrome metric is the standard deviation of hue weighted by saturation
    # take negative so high value means a high degree of monotonicity
    basic_metrics["Hmon_std"] = -weighted_circular_std_deg(h, s)

    # measure the degree to which the hue is close to each of the 6 cardinal hues
    # take negative so high value means high presencee of that color
    basic_metrics["H000_std"] = -np.mean((((h + 180 - 0) % 360) - 180)**2)**(1/2)
    basic_metrics["H060_std"] = -np.mean((((h + 180 - 60) % 360) - 180)**2)**(1/2)
    basic_metrics["H120_std"] = -np.mean((((h + 180 - 120) % 360) - 180)**2)**(1/2)
    basic_metrics["H180_std"] = -np.mean((((h + 180 - 180) % 360) - 180)**2)**(1/2)    
    basic_metrics["H240_std"] = -np.mean((((h + 180 - 240) % 360) - 180)**2)**(1/2)  
    basic_metrics["H300_std"] = -np.mean((((h + 180 - 300) % 360) - 180)**2)**(1/2)

    return basic_metrics

@timing_decorator("compute_lucas_kanade_metrics")
def compute_lucas_kanade_metrics(color_channel, color_channel_prior, center_region_ratio=1.0, downscale_factor=None):
    """
    Compute Lucas-Kanade optical flow metrics between two color channels.
    Extracts zoom and rotation information from the flow field using a centered region.
    
    Parameters:
    - color_channel: Current frame color channel
    - color_channel_prior: Previous frame color channel  
    - center_region_ratio: Size of centered region as fraction of shorter dimension (0.5 = 50% of shorter side)
    - downscale_factor: Factor to downscale the centered region (None = no downscaling)
    
    Returns:
    - Dictionary of flow-based metrics
    """
    # Convert to float32 for optical flow
    curr = color_channel.astype(np.float32)
    prev = color_channel_prior.astype(np.float32)
    
    # Calculate optical flow using Lucas-Kanade on full resolution
    flow = cv2.calcOpticalFlowFarneback(
        prev, curr, 
        None,  # No initial flow
        pyr_scale=0.5,  # Pyramid scale
        levels=3,       # Number of pyramid levels
        winsize=15,     # Window size
        iterations=3,   # Iterations
        poly_n=5,       # Polynomial degree
        poly_sigma=1.2, # Gaussian sigma
        flags=0
    )
    
    # Extract flow components
    flow_x = flow[:, :, 0]  # Horizontal flow
    flow_y = flow[:, :, 1]  # Vertical flow
    
    # Calculate center of image
    center_y, center_x = flow.shape[0] // 2, flow.shape[1] // 2
    
    # Calculate size of centered region
    shorter_dim = min(flow.shape[0], flow.shape[1])
    region_size = int(shorter_dim * center_region_ratio)
    
    # Calculate region boundaries
    start_y = center_y - region_size // 2
    end_y = center_y + region_size // 2
    start_x = center_x - region_size // 2
    end_x = center_x + region_size // 2
    
    # Ensure boundaries are within image
    start_y = max(0, start_y)
    end_y = min(flow.shape[0], end_y)
    start_x = max(0, start_x)
    end_x = min(flow.shape[1], end_x)
    
    # Extract centered region
    flow_x_center = flow_x[start_y:end_y, start_x:end_x]
    flow_y_center = flow_y[start_y:end_y, start_x:end_x]
    
    # Downscale the centered region if requested
    if downscale_factor is not None:
        h_center, w_center = flow_x_center.shape
        h_small = h_center // downscale_factor
        w_small = w_center // downscale_factor
        flow_x_center = cv2.resize(flow_x_center, (w_small, h_small))
        flow_y_center = cv2.resize(flow_y_center, (w_small, h_small))
    
    # Create coordinate grids for the centered region (after downscaling if applied)
    h_center, w_center = flow_x_center.shape
    y_coords, x_coords = np.meshgrid(
        np.arange(h_center), 
        np.arange(w_center), 
        indexing='ij'
    )
    
    # Calculate relative positions from center
    rel_x = x_coords - center_x
    rel_y = y_coords - center_y
    
    # Calculate distance from center
    distance_from_center = np.sqrt(rel_x**2 + rel_y**2)
    
    # Zoom metrics (divergence of flow) - only in centered region
    # Positive divergence = zoom out, negative = zoom in
    # Divergence = ∂u/∂x + ∂v/∂y (approximated using finite differences)
    du_dx = np.gradient(flow_x_center, axis=1)
    dv_dy = np.gradient(flow_y_center, axis=0)
    zoom_divergence = float(np.mean(du_dx + dv_dy))
    
    # Rotation metrics (curl of flow) - only in centered region
    # Positive curl = counterclockwise rotation, negative = clockwise
    # Curl = ∂v/∂x - ∂u/∂y (approximated using finite differences)
    dv_dx = np.gradient(flow_y_center, axis=1)
    du_dy = np.gradient(flow_x_center, axis=0)
    rotation_curl = float(np.mean(dv_dx - du_dy))
    
    # Overall motion magnitude - full image
    motion_magnitude = float(np.mean(np.sqrt(flow_x**2 + flow_y**2)))
    
    # Motion direction (angle) - full image
    motion_angle = float(np.arctan2(np.mean(flow_y), np.mean(flow_x)))
    
    # Radial vs tangential motion - only in centered region
    # Radial: motion toward/away from center (projection onto radial direction)
    # Unit radial vector = (rel_x, rel_y) / distance_from_center
    unit_radial_x = rel_x / (distance_from_center + 1e-6)
    unit_radial_y = rel_y / (distance_from_center + 1e-6)
    radial_motion = float(np.mean(flow_x_center * unit_radial_x + flow_y_center * unit_radial_y))
    
    # Tangential: motion perpendicular to radius (projection onto tangential direction)
    # Unit tangential vector = (-rel_y, rel_x) / distance_from_center (perpendicular to radial)
    unit_tangential_x = -rel_y / (distance_from_center + 1e-6)
    unit_tangential_y = rel_x / (distance_from_center + 1e-6)
    tangential_motion = float(np.mean(flow_x_center * unit_tangential_x + flow_y_center * unit_tangential_y))
    
    # Motion variance (how uniform the motion is) - full image
    motion_variance = float(np.var(flow_x) + np.var(flow_y))
    
    return {
        'czd': zoom_divergence,      # Zoom divergence
        'crc': rotation_curl,        # Rotation curl  
        'cmm': motion_magnitude,   # Overall motion magnitude
        'cma': motion_angle,       # Motion direction angle
        'crm': radial_motion,      # Radial motion component
        'ctm': tangential_motion,    # Tangential motion component
        'cmv': motion_variance     # Motion variance
    }

def compute_change_metrics(frame, frame_prior, downscale_large, downscale_medium):
    """
    Compute metrics that measure rates of change between consecutive frames.
    These metrics capture temporal dynamics and motion information.
    
    Parameters:
    - frame: Current frame
    - frame_prior: Previous frame
    - downscale_large: Large scale analysis factor
    - downscale_medium: Medium scale analysis factor
    
    Returns:
    - Dictionary of change metrics
    """
    change_metrics = {}
    
    # Split frames into color channels
    b, g, r = cv2.split(frame)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    h, s, v = bgr_to_hsv(b, g, r)
    
    b_prior, g_prior, r_prior = cv2.split(frame_prior)
    gray_prior = cv2.cvtColor(frame_prior, cv2.COLOR_BGR2GRAY)
    h_prior, s_prior, v_prior = bgr_to_hsv(b_prior, g_prior, r_prior)
    
    # Process each color channel
    for color_channel_name, color_channel, color_channel_prior in [
        ("R", r, r_prior), ("G", g, g_prior), ("B", b, b_prior),
        ("Gray", gray, gray_prior), ("S", s, s_prior), ("V", v, v_prior)
    ]:

        # rotational and zoom metrics should be the same for all color channels, 
        # so we only need to do it for the Gray channel
        if color_channel_name == "Gray":
        # Compute Lucas-Kanade metrics for this color channel
            lk_metrics = compute_lucas_kanade_metrics(color_channel, color_channel_prior, 1.0, 2)
            
            # Add color channel prefix to metric names
            for metric_name, metric_value in lk_metrics.items():
                change_metrics[f"{color_channel_name}_{metric_name}"] = metric_value
    
    return change_metrics

# Export metrics to CSV
def export_metrics_to_csv(frame_count_list, metrics, filename):
    """
    Export frame count and metric data to a CSV file using pandas,
    with metrics sorted alphabetically by key.

    Parameters:
    - frame_count_list (list): List of frame counts.
    - metrics (dict): Dictionary where each value is a list of the same length as frame_count_list.
    - filename (str): Name of the CSV file to write.
    """

    # Sort the metric keys alphabetically
    sorted_keys = sorted(metrics.keys())

    # Create a DataFrame using the sorted keys
    data = {'frame_count_list': frame_count_list}
    for key in sorted_keys:
        data[key] = metrics[key]

    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)

def process_video_to_csv(video_path, 
                          subdir_name, # output prefix 
                          frames_per_second, 
                          beats_per_midi_event,
                          ticks_per_beat, 
                          beats_per_minute, 
                          downscale_large,
                          downscale_medium):
    """
    Process every Nth frame, calculate metrics, and generate multiple MIDI files.
    
    :param video_path: Path to the video file.
    :param output_prefix: Prefix for output MIDI filenames.
    :param frames_per_second (number of frames per second in video)
    :param beats_per_midi_event (number of beats between each frame that will per processed)
    :param ticks_per_beat (number of midi ticks per beat in DAW)
    :param beats_per_minute (number of beats per minute in DAW)
    :param cc_number: MIDI CC number (default 7 for volume).
    :param channel: MIDI channel (0-15).
    :param downscale_large: spatial scale for computing metrics
    :param downscale_medium: resolution reduction for computing metrics
    :param filter_width: width of boxcar filter for smoothing

    """




    ticks_per_frame = ticks_per_beat *( beats_per_minute / 60.) / frames_per_second # ticks per second / frames per second
    # Calculate the frame interval for processing frames
    seconds_per_analysis_frame = beats_per_midi_event / (beats_per_minute / 60) # beats per frame / beats per second
    frames_per_analysis_frame_real = seconds_per_analysis_frame * frames_per_second
    # Take every Nth frame, where frames_per_interval_real is the floating point non-integer version of N

    frame_count = 0
    frame_count_list = []
    frame_prior = None  # Store immediately previous frame for change metrics

    # Initialize empty metrics dictionary - will be populated dynamically
    basic_metrics = {}
    
    # Change metrics will be added directly to basic_metrics dictionary

    # open the video file
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video file.")
        return
    
    # Get total number of frames in the video
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"Total frames in video: {total_frames}")
    
    # Set total start time for timing
    timing_data['total_start_time'] = time.time()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Always store the current frame for change metrics (even if we don't process it)
        if frame_prior is None:
            frame_prior = frame.copy()
        
        k = frame_count / frames_per_analysis_frame_real
        k_rounded = round(k)
        frame_count_good = round(k_rounded * frames_per_analysis_frame_real)
        if frame_count == frame_count_good or frame_count == total_frames - 1:
            print ("Processing frame:", frame_count)
            frame_count_list.append(frame_count)
            timing_data['frame_count'] += 1

            metric_results = compute_basic_metrics(frame, downscale_large, downscale_medium)

            # Compute change metrics with the immediately previous frame
            change_results = compute_change_metrics(frame, frame_prior, downscale_large, downscale_medium)
            
            # Add change metrics directly to basic_metrics
            for key, value in change_results.items():
                if key not in basic_metrics:
                    basic_metrics[key] = []
                basic_metrics[key].append(value)

            # Append basic metrics to the dictionary of results
            for key, value in metric_results.items():
                if key not in basic_metrics:
                    basic_metrics[key] = []
                basic_metrics[key].append(value)
        
        # Always update frame_prior to the current frame for next iteration
        frame_prior = frame.copy()
        frame_count += 1

    cap.release()

    # Set total end time for timing
    timing_data['total_end_time'] = time.time()
    # Print timing summary
    print_timing_summary()

    #now compute derivative metrics that are computed after all frames are processed
    basic_metrics["Hmon_std"] = np.array(basic_metrics["Hmon_std"])
    if np.max(basic_metrics["Hmon_std"]) == 0.0:
        diff_monos = 0.0
    else:
        diff_monos =   1.0 - basic_metrics["Hmon_std"] / np.max(basic_metrics["Hmon_std"])

    for key in ["H000_std", "H060_std", "H120_std", "H180_std", "H240_std", "H300_std"]:
        basic_metrics[key] = np.array(basic_metrics[key])
        # replace trailing s in key with i      
        key_i = key.replace("_std", "_int")  # i for intensity !
        basic_metrics[key_i] = (180 - basic_metrics[key]) * diff_monos

    # Export metrics to CSV
    csv_filename = f"{subdir_name}_basic.csv"
    export_metrics_to_csv(frame_count_list, basic_metrics, csv_filename)
    print(f"Metrics exported to {csv_filename}")

    # Write config to JSON for downstream use
    config = {
        "video_file": video_path,
        "subdir_name": subdir_name,
        "frames_per_second": frames_per_second,
        "beats_per_midi_event": beats_per_midi_event,
        "ticks_per_beat": ticks_per_beat,
        "beats_per_minute": beats_per_minute,
        "downscale_large": downscale_large,
        "downscale_medium": downscale_medium
    }
    config_filename = f"{subdir_name}_config.json"
    with open(config_filename, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"Config exported to {config_filename}")

    




# This script is designed to be called by run_video_processing.py
# For standalone usage, use: python run_video_processing.py <video_name>

